# -*- coding: utf-8 -*-
"""suicide_detection_model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I7KQuEsGm7_voIh7Sqq96m3Q5QPpVUdt
"""

# general libraries
import numpy as np
import pandas as pd
import nltk
import re
import string
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import json

# ml algorithms/tools
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')

#gathering data
labels_data_frame = pd.read_csv('/content/drive/MyDrive/Research/ML Research/SummerMLProject/twitter_suicide_data.csv')
labels = labels_data_frame["intention"].values

features_data_frame = pd.read_csv('/content/drive/MyDrive/Research/ML Research/SummerMLProject/features.csv')
features = features_data_frame[["suicidal","non-suicidal", "length"]].values

# splitting into training and validation set
x_training, x_validation, y_training, y_validation = train_test_split(features, labels, test_size = 0.25, random_state = 4)

# scaling the features using a standard scaler
scaler = preprocessing.StandardScaler()
scaler.fit(x_training)
x_training_scaled = scaler.transform(x_training)
x_validation_scaled = scaler.transform(x_validation)
print("Mean: ", np.mean(x_training))
print("Standard deviation: ", np.std(x_training))

# scaling the features using a min-max scaler
scaler = preprocessing.MinMaxScaler()
scaler.fit(x_training)
x_training_scaled = scaler.transform(x_training)
x_validation_scaled = scaler.transform(x_validation)

# creating the logistic regression model
model = LogisticRegression()
model.fit(x_training_scaled, y_training)

# calculating accuracy on training set
y_training_hat_cat = 1*(model.predict(x_training_scaled) > 0.5)
print(classification_report(y_training, y_training_hat_cat))

# caluclating accuray on valdiation set
y_validation_hat_cat = 1*(model.predict(x_validation_scaled) > 0.5)
print(classification_report(y_validation, y_validation_hat_cat))

# creating the decision tree model
model = DecisionTreeClassifier()
model.fit(x_training_scaled, y_training)

# calculating accuracy on training set
y_training_hat_cat = 1*(model.predict(x_training_scaled) > 0.5)
print(classification_report(y_training, y_training_hat_cat))

# caluclating accuray on valdiation set
y_validation_hat_cat = 1*(model.predict(x_validation_scaled) > 0.5)
print(classification_report(y_validation, y_validation_hat_cat))

# creating a support vector machine model
from sklearn.svm import SVC
model = SVC()
model.fit(x_training_scaled, y_training)

# calculating accuracy on training set
y_training_hat_cat = 1*(model.predict(x_training_scaled) > 0.5)
print(classification_report(y_training, y_training_hat_cat))

# caluclating accuray on valdiation set
y_validation_hat_cat = 1*(model.predict(x_validation_scaled) > 0.5)
print(classification_report(y_validation, y_validation_hat_cat))

# creating a naive bayes model
model = MultinomialNB()
model.fit(x_training_scaled, y_training)

# calculating accuracy on training set
y_training_hat_cat = 1*(model.predict(x_training_scaled) > 0.5)
print(classification_report(y_training, y_training_hat_cat))

# caluclating accuray on valdiation set
y_validation_hat_cat = 1*(model.predict(x_validation_scaled) > 0.5)
print(classification_report(y_validation, y_validation_hat_cat))

# creating a KNN model
model = KNeighborsClassifier()
model.fit(x_training_scaled, y_training)

# calculating accuracy on training set
y_training_hat_cat = 1*(model.predict(x_training_scaled) > 0.5)
print(classification_report(y_training, y_training_hat_cat))

# caluclating accuray on valdiation set
y_validation_hat_cat = 1*(model.predict(x_validation_scaled) > 0.5)
print(classification_report(y_validation, y_validation_hat_cat))

# user input
tweet = input("Tweet: ")
proccessed_tweets = []

# tokenizing the tweets
tokenizer = TweetTokenizer()  
tokenized_list = tokenizer.tokenize(tweet)

for i in range(len(tokenized_list)):
  proccessed_tweets.append(tokenized_list[i])

# removing stop words
nltk.download('stopwords')
stop_words = stopwords.words('english')

for i in range(len(proccessed_tweets)):
  for n in range(len(stop_words)):
    if proccessed_tweets[i] == stop_words[n]:
      proccessed_tweets[i] = re.sub(r"\b{}\b".format(stop_words[n]),"", proccessed_tweets[i])

# removing punctuation
# all standard punctuation has been already removed from the dataset, but emoticons are still present
punctuation_string = string.punctuation
punctuation = tokenizer.tokenize(punctuation_string)

for i in range(len(proccessed_tweets)):
  for n in range(len(punctuation)):
    if proccessed_tweets[i] == punctuation[n]:
      proccessed_tweets[i] = re.sub(r"\b{}\b".format(punctuation[n]),"", proccessed_tweets[i])

final_tweet = []

# stemming words
stemmer = PorterStemmer()
for i in range(len(proccessed_tweets)):
  pre_stemmed = proccessed_tweets[i]
  stemmed_word = stemmer.stem(pre_stemmed)
  final_tweet.append(stemmed_word)
  
# removing white spaces
blank = 0
for i in range(len(final_tweet)):
  if final_tweet[i] == "":
    blank += 1

for i in range(blank):
  final_tweet.remove("")

# turning features JSON file into dictionary
with open('/content/drive/MyDrive/Research/ML Research/SummerMLProject/frequency.json') as json_file:
  frequency = json.load(json_file)

total_suicidal_value = 0
total_non_suicidal_value = 0

for word in final_tweet:

  suicidal_value = 0
  non_suicidal_value = 0

  suicidal_key = word + " ,1"
  non_suicidal_key = word + " ,0"

  if suicidal_key in frequency:
    suicidal_value = frequency[suicidal_key]
  if non_suicidal_key in frequency:
    non_suicidal_value = frequency[non_suicidal_key]
    
  total_suicidal_value += suicidal_value
  total_non_suicidal_value += non_suicidal_value

features_input = [total_suicidal_value, total_non_suicidal_value, len(tweet)]
  
features_input = np.array(features_input)
features_input = features_input.reshape(-1,3)

x_input = scaler.transform(features_input)

print()
print()

predicted_output = model.predict(x_input)
if predicted_output >= 0.5:
  print("suicidal, exact probability: ", predicted_output)
else:
  print("non-suicidal, exact probability: ", predicted_output)

print()
print()

# turning into usable file for aplication

!pip install --upgrade pip
!pip install -U coremltools

from coremltools.converters import sklearn as sklearn_to_ml

print('Converting model')
coreml_model = sklearn_to_ml.convert(model)

print('Saving CoreML model')
coreml_model.save('suicideDetectionModel.mlmodel')